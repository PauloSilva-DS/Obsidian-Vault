---
tags:
  - estudo
  - python
  - AprendizadoMaquina
  - √°lgebraLinear
Completo: false
Atualizado: 2025-03-13  11.42
Criado: 2025-03-11  15.38
---
üîñ[[Aprendizado de m√°quina]]

---

**Matem√°tica - √Ålgebra Linear**

A √Ålgebra Linear √© o ramo da matem√°tica que estuda espa√ßos vetoriais e transforma√ß√µes lineares entre espa√ßos vetoriais, como girar uma forma, escalon√°-la (aumentar ou diminuir seu tamanho), translad√°-la (ou seja, mov√™-la), etc.

O Aprendizado de M√°quina depende fortemente da √Ålgebra Linear, por isso √© essencial entender o que s√£o vetores e matrizes, quais opera√ß√µes podem ser realizadas com eles e como eles podem ser √∫teis.


### ‚úî **Vetores**

**Defini√ß√£o**

Um vetor √© uma quantidade definida por uma magnitude e uma dire√ß√£o. Por exemplo, a velocidade de um foguete √© um vetor tridimensional: sua magnitude √© a velocidade do foguete, e sua dire√ß√£o √© (esperan√ßosamente) para cima. Um vetor pode ser representado por um array de n√∫meros chamados escalares. Cada escalar corresponde √† magnitude do vetor em rela√ß√£o a cada dimens√£o.

Por exemplo, digamos que o foguete est√° subindo em um √¢ngulo leve: ele tem uma velocidade vertical de 5.000 m/s, e tamb√©m uma leve velocidade em dire√ß√£o ao Leste de 10 m/s, e uma leve velocidade em dire√ß√£o ao Norte de 50 m/s. A velocidade do foguete pode ser representada pelo seguinte vetor:
![[Pasted image 20250311154341.png]]

**Nota:** por conven√ß√£o, os vetores s√£o geralmente apresentados na forma de colunas. Al√©m disso, os nomes dos vetores s√£o geralmente em letras min√∫sculas para distingui-los das matrizes (que discutiremos abaixo) e em negrito (quando poss√≠vel) para distingui-los de valores escalares simples, como <font color="#000000">`metros_por_segundo = 5026`.</font>

Uma lista de N n√∫meros tamb√©m pode representar as coordenadas de um ponto em um espa√ßo N-dimensional, por isso √© bastante frequente representar vetores como pontos simples em vez de setas. Um vetor com 1 elemento pode ser representado como uma seta ou um ponto em um eixo, um vetor com 2 elementos √© uma seta ou um ponto em um plano, um vetor com 3 elementos √© uma seta ou um ponto no espa√ßo, e um vetor com N elementos √© uma seta ou um ponto em um espa√ßo N-dimensional... o que a maioria das pessoas acha dif√≠cil de imaginar.

---

### **Prop√≥sito**

Os vetores t√™m muitos prop√≥sitos no Aprendizado de M√°quina, principalmente para representar observa√ß√µes e previs√µes. Por exemplo, digamos que constru√≠mos um sistema de Aprendizado de M√°quina para classificar v√≠deos em 3 categorias (bom, spam, clickbait) com base no que sabemos sobre eles. Para cada v√≠deo, ter√≠amos um vetor representando o que sabemos sobre ele, como:

![[Pasted image 20250311154907.png]]

Esse vetor poderia representar um v√≠deo que dura 10,5 minutos, mas apenas 5,2% dos espectadores assistem por mais de um minuto, ele recebe 3,25 visualiza√ß√µes por dia em m√©dia e foi sinalizado 7 vezes como spam. Como voc√™ pode ver, cada eixo pode ter um significado diferente.

Com base nesse vetor, nosso sistema de Aprendizado de M√°quina pode prever que h√° uma probabilidade de 80% de ser um v√≠deo spam, 18% de ser clickbait e 2% de ser um bom v√≠deo. Isso pode ser representado como o seguinte vetor:

![[Pasted image 20250311155314.png]]


---

### **‚úÖ Vetores em Python**

Em Python, um vetor pode ser representado de v√°rias maneiras, a mais simples sendo uma lista regular de n√∫meros:

```run-python
[10.5, 5.2, 3.25, 7.0]
```


Como planejamos fazer muitos c√°lculos cient√≠ficos, √© muito melhor usar o `ndarray` do NumPy, que fornece muitas implementa√ß√µes convenientes e otimizadas de opera√ß√µes matem√°ticas essenciais em vetores (para mais detalhes sobre o NumPy, confira o tutorial do NumPy). Por exemplo:

```run-python
import numpy as np

video = np.array([10.5, 5.2, 3.25, 7.0])
print(video)
```

---

### **Tamanho de um vetor**

O tamanho de um vetor pode ser obtido usando o atributo `size`:

```run-python
video.size
```

O $i^{th}$ elemento (tamb√©m chamado de entrada ou item) de um vetor $\textbf{v}$  √© denotado por $\textbf{v}_i$..



Observe que os √≠ndices em matem√°tica geralmente come√ßam em 1, mas na programa√ß√£o eles geralmente come√ßam em 0. Portanto, para acessar o terceiro elemento de `video` programaticamente, escrever√≠amos:

```run-python
video[2]  # 3¬∫ elemento
```

---

### **Plotando vetores**

Para plotar vetores, usaremos o `matplotlib`, ent√£o vamos come√ßar importando-o (para detalhes sobre o `matplotlib`, confira o tutorial do `matplotlib`):

```run-python
import matplotlib.pyplot as plt
```

---

### **Vetores 2D**

Vamos criar alguns vetores 2D muito simples para plotar:

```run-python
u = np.array([2, 5])
v = np.array([3, 1])
```

Esses vetores t√™m 2 elementos cada, ent√£o podem ser facilmente representados graficamente em um gr√°fico 2D, por exemplo, como pontos:

```run-python
x_coords, y_coords = zip(u, v)
plt.scatter(x_coords, y_coords, color=["r","b"])
plt.axis([0, 9, 0, 6])
plt.grid()
plt.show()
```

Os vetores tamb√©m podem ser representados como setas. Vamos criar uma pequena fun√ß√£o de conveni√™ncia para desenhar setas bonitas:

```run-python
def plot_vector2d(vector2d, origin=[0, 0], **options):
    return plt.arrow(origin[0], origin[1], vector2d[0], vector2d[1],
                     head_width=0.2, head_length=0.3, length_includes_head=True,
                     **options)
```

Agora vamos desenhar os vetores `u` e `v` como setas:

```run-python
plot_vector2d(u, color="r")
plot_vector2d(v, color="b")
plt.axis([0, 9, 0, 6])
plt.grid()
plt.show()
```


---

### **Vetores 3D**

Plotar vetores 3D tamb√©m √© relativamente simples. Primeiro, vamos criar dois vetores 3D:

```run-python
a = np.array([1, 2, 8])
b = np.array([5, 6, 3])


```

Agora vamos plot√°-los usando o `Axes3D` do `matplotlib`:


```run-python
import matplotlib.pyplot as plt
import numpy as np
subplot3d = plt.subplot(111, projection='3d')
x_coords, y_coords, z_coords = zip(a,b)
subplot3d.scatter(x_coords, y_coords, z_coords)
subplot3d.set_zlim3d([0, 9])
plt.savefig('/home/paulo/Obsidian Vault/Obsidian Vault/plot/plot3d.png')

```

![[plot3d.png|500]]







√â um pouco dif√≠cil visualizar exatamente onde no espa√ßo esses dois pontos est√£o, ent√£o vamos adicionar linhas verticais.

Vamos criar uma pequena fun√ß√£o de conveni√™ncia para plotar uma lista de vetores 3D com linhas verticais anexadas:

```run-python
def plot_vectors3d(ax, vectors3d, z0, **options):
    for v in vectors3d:
        x, y, z = v
        ax.plot([x, x], [y, y], [z0, z], color="gray", linestyle='dotted', marker=".")
    x_coords, y_coords, z_coords = zip(*vectors3d)
    ax.scatter(x_coords, y_coords, z_coords, **options)

subplot3d = plt.subplot(111, projection='3d')
subplot3d.set_zlim([0, 9])
plot_vectors3d(subplot3d, [a, b], 0, color=("r","b"))
#plt.show()
plt.savefig('/home/paulo/Obsidian Vault/Obsidian Vault/plot/plot3d.png')
```


![[plot3d.png|500]]




---

### **Norma**

A norma de um vetor $( \mathbf{u} )$, denotada por $( \|\mathbf{u}\| )$, √© uma medida do comprimento (tamb√©m conhecida como magnitude) de $( \mathbf{u} )$.

Existem v√°rias normas poss√≠veis, mas a mais comum (e a √∫nica que discutiremos aqui) √© a norma Euclidiana, que √© definida como:


$\|\mathbf{u}\| = \sqrt{\sum_{i} \mathbf{u}_{i}^{2}}$


Isso √© a raiz quadrada da soma dos quadrados de todos os componentes de $( \mathbf{u} )$. Poder√≠amos implementar isso facilmente em Python puro, lembrando que $( \sqrt{x} = x^{\frac{1}{2}} )$:

```run-python
def norma_euclidiana(u):
    return (sum(x**2 for x in u))**0.5

# Exemplo de uso
vetor = [3, 4]
resultado = norma_euclidiana(vetor)
print(f"A norma do vetor {vetor} √© {resultado}")

```




No entanto, √© muito mais eficiente usar a fun√ß√£o `norm` do NumPy, dispon√≠vel no m√≥dulo `linalg` (√Ålgebra Linear):

```run-python
import numpy.linalg as LA
u = np.array([2, 5]) 
print(LA.norm(u))
```






Vamos plotar um pequeno diagrama para confirmar que o comprimento do vetor $( \mathbf{u} )$ √© de fato $( \approx 5,4 )$:



```run-python
radius = LA.norm(u)
plt.gca().add_artist(plt.Circle((0,0), radius, color="#DDDDDD"))
plot_vector2d(u, color="red")
plt.axis([0, 8.7, 0, 6])
plt.gca().set_aspect("equal")
plt.grid()
plt.savefig('/home/paulo/Obsidian Vault/Obsidian Vault/plot/plot3d.png')
```

Parece correto!

---

### **Adi√ß√£o**

Vetores de mesmo tamanho podem ser somados. A adi√ß√£o √© realizada _elemento por elemento_:

```run-python
print(" ", u)
print("+", v)
print("-"*10)
print(u + v)
```


Vamos ver como a adi√ß√£o de vetores se parece graficamente:

```run-python
plot_vector2d(u, color="r")
plot_vector2d(v, color="b")
plot_vector2d(v, origin=u, color="b", linestyle="dotted")
plot_vector2d(u, origin=v, color="r", linestyle="dotted")
plot_vector2d(u+v, color="g")
plt.axis([0, 9, 0, 7])
plt.gca().set_aspect("equal")
plt.text(0.7, 3, "u", color="r", fontsize=18)
plt.text(4, 3, "u", color="r", fontsize=18)
plt.text(1.8, 0.2, "v", color="b", fontsize=18)
plt.text(3.1, 5.6, "v", color="b", fontsize=18)
plt.text(2.4, 2.5, "u+v", color="g", fontsize=18)
plt.grid()
plt.show()
```

A adi√ß√£o de vetores √© **comutativa**, o que significa que $( \mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u} )$. Voc√™ pode ver isso na imagem anterior: seguir$( \mathbf{u} )$ e depois $( \mathbf{v} )$ leva ao mesmo ponto que seguir $( \mathbf{v} )$ e depois $( \mathbf{u} )$.

A adi√ß√£o de vetores tamb√©m √© **associativa**, o que significa que $( \mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w} )$.

Se voc√™ tiver uma forma definida por um n√∫mero de pontos (vetores) e adicionar um vetor $( \mathbf{v})$ a todos esses pontos, ent√£o toda a forma ser√° deslocada por $( \mathbf{v} )$. Isso √© chamado de transla√ß√£o geom√©trica:

```run-python
t1 = np.array([2, 0.25])
t2 = np.array([2.5, 3.5])
t3 = np.array([1, 2])

x_coords, y_coords = zip(t1, t2, t3, t1)
plt.plot(x_coords, y_coords, "c--", x_coords, y_coords, "co")

plot_vector2d(v, t1, color="r", linestyle=":")
plot_vector2d(v, t2, color="r", linestyle=":")
plot_vector2d(v, t3, color="r", linestyle=":")

t1b = t1 + v
t2b = t2 + v
t3b = t3 + v

x_coords_b, y_coords_b = zip(t1b, t2b, t3b, t1b)
plt.plot(x_coords_b, y_coords_b, "b--", x_coords_b, y_coords_b, "bo")

plt.text(4, 4.2, "v", color="r", fontsize=18)
plt.text(3, 2.3, "v", color="r", fontsize=18)
plt.text(3.5, 0.4, "v", color="r", fontsize=18)

plt.axis([0, 6, 0, 5])
plt.gca().set_aspect("equal")
plt.grid()
plt.show()
```

Finalmente, subtrair um vetor √© como adicionar o vetor oposto.

---

### **Multiplica√ß√£o por um escalar**

Vetores podem ser multiplicados por escalares. Todos os elementos do vetor s√£o multiplicados por esse n√∫mero, por exemplo:

```run-python
print("1.5 *", u, "=")
print(1.5 * u)
```

Graficamente, a multiplica√ß√£o por escalar resulta na mudan√ßa da escala de uma figura, da√≠ o nome _escalar_. A dist√¢ncia da origem (o ponto nas coordenadas iguais a zero) tamb√©m √© multiplicada pelo escalar. Por exemplo, vamos aumentar a escala por um fator de k = 2,5:

```run-python
k = 2.5
t1c = k * t1
t2c = k * t2
t3c = k * t3

plt.plot(x_coords, y_coords, "c--", x_coords, y_coords, "co")

plot_vector2d(t1, color="r")
plot_vector2d(t2, color="r")
plot_vector2d(t3, color="r")

x_coords_c, y_coords_c = zip(t1c, t2c, t3c, t1c)
plt.plot(x_coords_c, y_coords_c, "b-", x_coords_c, y_coords_c, "bo")

plot_vector2d(k * t1, color="b", linestyle=":")
plot_vector2d(k * t2, color="b", linestyle=":")
plot_vector2d(k * t3, color="b", linestyle=":")

plt.axis([0, 9, 0, 9])
plt.gca().set_aspect("equal")
plt.grid()
plt.show()
```

Como voc√™ pode imaginar, dividir um vetor por um escalar √© equivalente a multiplicar pelo seu inverso multiplicativo (rec√≠proco):

$$

\frac{\mathbf{u}}{\lambda} = \frac{1}{\lambda} \times \mathbf{u}
$$



A multiplica√ß√£o por escalar √© **comutativa**: $$( \lambda \times \mathbf{u} = \mathbf{u} \times \lambda).

$$

Tamb√©m √© **associativa**:$$ ( \lambda_1 \times (\lambda_2 \times \mathbf{u}) = (\lambda_1 \times \lambda_2) \times \mathbf{u} ).
$$

Finalmente, √© **distributiva** sobre a adi√ß√£o de vetores:
$$
( \lambda \times (\mathbf{u} + \mathbf{v}) = \lambda \times \mathbf{u} + \lambda \times \mathbf{v} ).
$$

---

### **Vetores nulos, unit√°rios e normalizados**

- Um **vetor nulo** √© um vetor cheio de 0s.
- Um **vetor unit√°rio** √© um vetor com norma igual a 1.
- O **vetor normalizado** de um vetor n√£o nulo $( \mathbf{v}),$ denotado por $( \hat{\mathbf{v}} )$, √© o vetor unit√°rio que aponta na mesma dire√ß√£o que $( \mathbf{v}).$ Ele √© igual a: $( \hat{\mathbf{v}} = \frac{\mathbf{v}}{\|\mathbf{v}\|}).$
-

```run-python
plt.gca().add_artist(plt.Circle((0, 0), 1, color='c'))
plt.plot(0, 0, "ko")
plot_vector2d(v / LA.norm(v), color="k", zorder=10)
plot_vector2d(v, color="b", linestyle=":", zorder=15)
plt.text(0.3, 0.3, r"$\hat{v}$", color="k", fontsize=18)
plt.text(1.5, 0.7, "$v$", color="b", fontsize=18)
plt.axis([-1.5, 5.5, -1.5, 3.5])
plt.gca().set_aspect("equal")
plt.grid()
plt.show()
```

---

### **Produto Escalar**

**Defini√ß√£o**

O produto escalar (tamb√©m chamado de _produto interno_ no contexto do espa√ßo Euclidiano) de dois vetores ( $\mathbf{u}$ ) e $( \mathbf{v} )$ √© uma opera√ß√£o √∫til que aparece frequentemente em √°lgebra linear. Ele √© denotado por $( \mathbf{u} \cdot \mathbf{v} )$, ou √†s vezes $( \langle \mathbf{u}|\mathbf{v} \rangle )$ ou $( (\mathbf{u}|\mathbf{v}) )$, e √© definido como:


$\mathbf{u} \cdot \mathbf{v} = \|\mathbf{u}\| \times \|\mathbf{v}\| \times \cos(\theta)$


onde $( \theta )$ √© o √¢ngulo entre $( \mathbf{u} ) e ( \mathbf{v}).$

Outra maneira de calcular o produto escalar √©:



$\mathbf{u} \cdot \mathbf{v} = \sum_{i} u_i \times v_]$

**Em Python**

O produto escalar √© bastante simples de implementar:

```run-python
def dot_product(v1, v2):
    return sum(v1_i * v2_i for v1_i, v2_i in zip(v1, v2))

dot_product(u, v)
```

Mas uma implementa√ß√£o muito mais eficiente √© fornecida pelo NumPy com a fun√ß√£o `np.dot()`:

```run-python
np.dot(u, v)
```

Equivalentemente, voc√™ pode usar o m√©todo `dot` dos `ndarray`s:

```run-python
u.dot(v)
```

**Cuidado:** o operador `*` realizar√° uma multiplica√ß√£o elemento por elemento, **N√ÉO** um produto escalar:

```run-python
print(" ", u)
print("* ", v, "(N√ÉO √© um produto escalar)")
print("-"*10)
print(u * v)
```

**Propriedades principais**

- O produto escalar √© **comutativo**: $( \mathbf{u} \cdot \mathbf{v} = \mathbf{v} \cdot \mathbf{u} )$.
- O produto escalar √© definido apenas entre dois vetores, n√£o entre um escalar e um vetor. Isso significa que n√£o podemos encadear produtos escalares: por exemplo, a express√£o $( \mathbf{u} \cdot \mathbf{v} \cdot \mathbf{w} )$ n√£o √© definida, pois $( \mathbf{u} \cdot \mathbf{v} )$ √© um escalar e $( \mathbf{w} )$ √© um vetor.
- Isso tamb√©m significa que o produto escalar **N√ÉO** √© associativo: \( (\mathbf{u} \cdot \mathbf{v}) \cdot \mathbf{w} \neq \mathbf{u} \cdot (\mathbf{v} \cdot \mathbf{w}) \), j√° que nenhum dos dois √© definido.
- No entanto, o produto escalar √© **associativo em rela√ß√£o √† multiplica√ß√£o por escalar**: \( \lambda \times (\mathbf{u} \cdot \mathbf{v}) = (\lambda \times \mathbf{u}) \cdot \mathbf{v} = \mathbf{u} \cdot (\lambda \times \mathbf{v}) \).
- Finalmente, o produto escalar √© **distributivo** sobre a adi√ß√£o de vetores: \( \mathbf{u} \cdot (\mathbf{v} + \mathbf{w}) = \mathbf{u} \cdot \mathbf{v} + \mathbf{u} \cdot \mathbf{w} \).

---

### **Calculando o √¢ngulo entre vetores**

Um dos muitos usos do produto escalar √© calcular o √¢ngulo entre dois vetores n√£o nulos. Olhando para a defini√ß√£o do produto escalar, podemos deduzir a seguinte f√≥rmula:

\[
\theta = \arccos \left( \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \times \|\mathbf{v}\|} \right)
\]

Observe que se \( \mathbf{u} \cdot \mathbf{v} = 0 \), segue que \( \theta = \frac{\pi}{2} \). Em outras palavras, se o produto escalar de dois vetores n√£o nulos for zero, isso significa que eles s√£o ortogonais.

Vamos usar essa f√≥rmula para calcular o √¢ngulo entre \( \mathbf{u} \) e \( \mathbf{v} \) (em radianos):

```run-python
def vector_angle(u, v):
    cos_theta = u.dot(v) / LA.norm(u) / LA.norm(v)
    return np.arccos(cos_theta.clip(-1, 1))

theta = vector_angle(u, v)
print("√Çngulo =", theta, "radianos")
print("    =", theta * 180 / np.pi, "graus")
```

**Nota:** devido a pequenos erros de ponto flutuante, `cos_theta` pode estar ligeiramente fora do intervalo \([-1, 1]\), o que faria o `arccos` falhar. √â por isso que cortamos o valor dentro do intervalo usando a fun√ß√£o `clip` do NumPy.

---

### **Projetando um ponto em um eixo**

O produto escalar tamb√©m √© muito √∫til para projetar pontos em um eixo. A proje√ß√£o do vetor \( \mathbf{v} \) no eixo de \( \mathbf{u} \) √© dada por esta f√≥rmula:

\[
\text{proj}_{\mathbf{u}} \mathbf{v} = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\|^2} \times \mathbf{u}
\]

O que √© equivalente a:

\[
\text{proj}_{\mathbf{u}} \mathbf{v} = (\mathbf{v} \cdot \hat{\mathbf{u}}) \times \hat{\mathbf{u}}
\]

```run-python
u_normalized = u / LA.norm(u)
proj = v.dot(u_normalized) * u_normalized

plot_vector2d(u, color="r")
plot_vector2d(v, color="b")

plot_vector2d(proj, color="k", linestyle=":")
plt.plot(proj[0], proj[1], "ko")

plt.plot([proj[0], v[0]], [proj[1], v[1]], "b:")

plt.text(1, 2, "$proj_u v$", color="k", fontsize=18)
plt.text(1.8, 0.2, "$v$", color="b", fontsize=18)
plt.text(0.8, 3, "$u$", color="r", fontsize=18)

plt.axis([0, 8, 0, 5.5])
plt.gca().set_aspect("equal")
plt.grid()
plt.show()
```

---

### **Matrizes**

Uma matriz √© um array retangular de escalares (ou seja, qualquer n√∫mero: inteiro, real ou complexo) organizados em linhas e colunas, por exemplo:

\[
\begin{bmatrix}
10 & 20 & 30 \\
40 & 50 & 60
\end{bmatrix}
\]

Voc√™ tamb√©m pode pensar em uma matriz como uma lista de vetores: a matriz anterior cont√©m 2 vetores horizontais 3D ou 3 vetores verticais 2D.

As matrizes s√£o convenientes e muito eficientes para realizar opera√ß√µes em muitos vetores de uma s√≥ vez. Tamb√©m veremos que elas s√£o √≥timas para representar e realizar transforma√ß√µes lineares, como rota√ß√µes, transla√ß√µes e escalonamentos.

---

### **Matrizes em Python**

Em Python, uma matriz pode ser representada de v√°rias maneiras. A mais simples √© apenas uma lista de listas do Python:

```run-python
[
    [10, 20, 30],
    [40, 50, 60]
]
```

Uma maneira muito mais eficiente √© usar a biblioteca NumPy, que fornece implementa√ß√µes otimizadas de muitas opera√ß√µes matriciais:

```run-python
A = np.array([
    [10, 20, 30],
    [40, 50, 60]
])
A
```

Por conven√ß√£o, as matrizes geralmente t√™m nomes em mai√∫sculas, como \( A \).

No restante deste tutorial, assumiremos que estamos usando arrays NumPy (tipo `ndarray`) para representar matrizes.

---

### **Tamanho**

O tamanho de uma matriz √© definido pelo n√∫mero de linhas e colunas. Ele √© denotado por  
linhas √ó colunas. Por exemplo, a matriz \( A \) acima √© um exemplo de uma matriz \( 2 \times 3 \): 2 linhas, 3 colunas. **Cuidado:** uma matriz \( 3 \times 2 \) teria 3 linhas e 2 colunas.

Para obter o tamanho de uma matriz no NumPy:

```run-python
A.shape
```

**Cuidado:** o atributo `size` representa o n√∫mero de elementos no `ndarray`, n√£o o tamanho da matriz:

```run-python
A.size
```

---

### **Indexa√ß√£o de elementos**

O n√∫mero localizado na \( i^{√©sima} \) linha e \( j^{√©sima} \) coluna de uma matriz \( X \) √© √†s vezes denotado por \( X_{i,j} \) ou \( X_{ij} \), mas n√£o h√° uma nota√ß√£o padr√£o, ent√£o as pessoas geralmente preferem nomear explicitamente os elementos, assim: "seja \( X = (x_{i,j})_{1 \leq i \leq m, 1 \leq j \leq n} \)." Isso significa que \( X \) √© igual a:

\[
X = 
\begin{bmatrix}
x_{1,1} & x_{1,2} & x_{1,3} & \cdots & x_{1,n} \\
x_{2,1} & x_{2,2} & x_{2,3} & \cdots & x_{2,n} \\
x_{3,1} & x_{3,2} & x_{3,3} & \cdots & x_{3,n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
x_{m,1} & x_{m,2} & x_{m,3} & \cdots & x_{m,n}
\end{bmatrix}
\]

No entanto, neste notebook, usaremos a nota√ß√£o \( X_{i,j} \), pois ela corresponde bem √† nota√ß√£o do NumPy. Observe que, em matem√°tica, os √≠ndices geralmente come√ßam em 1, mas na programa√ß√£o eles geralmente come√ßam em 0. Portanto, para acessar \( A_{2,3} \) programaticamente, precisamos escrever:

```run-python
A[1, 2]  # 2¬™ linha, 3¬™ coluna
```

O vetor da \( i^{√©sima} \) linha √© √†s vezes denotado por \( M_i \) ou \( M_{i,*} \), mas novamente n√£o h√° uma nota√ß√£o padr√£o, ent√£o as pessoas geralmente preferem definir seus pr√≥prios nomes, por exemplo: "seja \( x_i \) o vetor da \( i^{√©sima} \) linha da matriz \( X \)." Usaremos \( M_{i,*} \) pelo mesmo motivo mencionado acima. Por exemplo, para acessar \( A_{2,*} \) (ou seja, o vetor da 2¬™ linha de \( A \)):

```run-python
A[1, :]  # vetor da 2¬™ linha (como um array 1D)
```

Da mesma forma, o vetor da \( j^{√©sima} \) coluna √© √†s vezes denotado por \( M^j \) ou \( M_{*,j} \), mas n√£o h√° uma nota√ß√£o padr√£o. Usaremos \( M_{*,j} \). Por exemplo, para acessar \( A_{*,3} \) (ou seja, o vetor da 3¬™ coluna de \( A \)):

```run-python
A[:, 2]  # vetor da 3¬™ coluna (como um array 1D)
```

Observe que o resultado √© na verdade um array NumPy unidimensional: n√£o existe algo como um array unidimensional vertical ou horizontal. Se voc√™ precisar realmente representar um vetor linha como uma matriz de uma linha (ou seja, um array 2D do NumPy) ou um vetor coluna como uma matriz de uma coluna, ent√£o voc√™ precisa usar um slice em vez de um inteiro ao acessar a linha ou coluna, por exemplo:

```run-python
A[1:2, :]  # linhas 2 a 3 (exclu√≠do): isso retorna a linha 2 como uma matriz de uma linha
A[:, 2:3]  # colunas 3 a 4 (exclu√≠do): isso retorna a coluna 3 como uma matriz de uma coluna
```

---

### **Matrizes quadradas, triangulares, diagonais e identidade**

Uma **matriz quadrada** √© uma matriz que tem o mesmo n√∫mero de linhas e colunas, por exemplo, uma matriz \( 3 \times 3 \):

$$

\begin{bmatrix}
4 & 9 & 2 \\
3 & 5 & 7 \\
8 & 1 & 6
\end{bmatrix}
$$


Uma **matriz triangular superior** √© um tipo especial de matriz quadrada onde todos os elementos abaixo da diagonal principal (do canto superior esquerdo para o canto inferior direito) s√£o zero, por exemplo:

$$

\begin{bmatrix}
4 & 9 & 2 \\
0 & 5 & 7 \\
0 & 0 & 6
\end{bmatrix}

$$

Da mesma forma, uma **matriz triangular inferior** √© uma matriz quadrada onde todos os elementos acima da diagonal principal s√£o zero, por exemplo:

$$

\begin{bmatrix}
4 & 0 & 0 \\
3 & 5 & 0 \\
8 & 1 & 6
\end{bmatrix}
$$


Uma **matriz triangular** √© aquela que √© triangular superior ou inferior.

Uma matriz que √© tanto triangular superior quanto inferior √© chamada de **matriz diagonal**, por exemplo:

$$

\begin{bmatrix}
4 & 0 & 0 \\
0 & 5 & 0 \\
0 & 0 & 6
\end{bmatrix}
$$


Voc√™ pode construir uma matriz diagonal usando a fun√ß√£o `diag` do NumPy:

```run-python
np.diag([4, 5, 6])
```

Se voc√™ passar uma matriz para a fun√ß√£o `diag`, ela extrair√° os valores da diagonal:

```run-python
D = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
])
print(np.diag(D))
```

Finalmente, a **matriz identidade** de tamanho \( n \), denotada por \( I_n \), √© uma matriz diagonal de tamanho \( n \times n \) com 1s na diagonal principal, por exemplo \( I_3 \):

\[
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\]

A fun√ß√£o `eye` do NumPy retorna a matriz identidade do tamanho desejado:

```run-python
np.eye(3)
```

A matriz identidade √© frequentemente denotada simplesmente por \( I \) (em vez de \( I_n \)) quando seu tamanho √© claro dado o contexto. Ela √© chamada de matriz identidade porque multiplicar uma matriz por ela deixa a matriz inalterada, como veremos abaixo.

---

### **Adi√ß√£o de matrizes**

Se duas matrizes \( Q \) e \( R \) tiverem o mesmo tamanho \( m \times n \), elas podem ser somadas. A adi√ß√£o √© realizada elemento por elemento: o resultado tamb√©m √© uma matriz \( m \times n \) \( S \) onde cada elemento √© a soma dos elementos na posi√ß√£o correspondente:

$$

S_{i,j} = Q_{i,j} + R_{i,j}
$$$$


S = 
\begin{bmatrix}
Q_{11} + R_{11} & Q_{12} + R_{12} & Q_{13} + R_{13} & \cdots & Q_{1n} + R_{1n} \\
Q_{21} + R_{21} & Q_{22} + R_{22} & Q_{23} + R_{23} & \cdots & Q_{2n} + R_{2n} \\
Q_{31} + R_{31} & Q_{32} + R_{32} & Q_{33} + R_{33} & \cdots & Q_{3n} + R_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
Q_{m1} + R_{m1} & Q_{m2} + R_{m2} & Q_{m3} + R_{m3} & \cdots & Q_{mn} + R_{mn}
\end{bmatrix}

$$

Por exemplo, vamos criar uma matriz \( 2 \times 3 \) \( B \) e calcular \( A + B \):

```run-python
B = np.array([
    [1, 2, 3],
    [4, 5, 6]
])
A = np.array([
       [10, 20, 30],
       [40, 50, 60]
])

A + B
```



A adi√ß√£o √© comutativa, o que significa que \( A + B = B + A \):

```run-python
B + A
```

Tamb√©m √© associativa, o que significa que \( A + (B + C) = (A + B) + C \):

```run-python
C = np.array([
    [100, 200, 300],
    [400, 500, 600]
])
A + (B + C)

(A + B) + C
```

---

### **Multiplica√ß√£o por escalar**

Uma matriz \( M \) pode ser multiplicada por um escalar \( \lambda \). O resultado √© denotado por \( \lambda M \), e √© uma matriz do mesmo tamanho que \( M \) com todos os elementos multiplicados por \( \lambda \):

\[
\lambda M = 
\begin{bmatrix}
\lambda \times M_{11} & \lambda \times M_{12} & \lambda \times M_{13} & \cdots & \lambda \times M_{1n} \\
\lambda \times M_{21} & \lambda \times M_{22} & \lambda \times M_{23} & \cdots & \lambda \times M_{2n} \\
\lambda \times M_{31} & \lambda \times M_{32} & \lambda \times M_{33} & \cdots & \lambda \times M_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\lambda \times M_{m1} & \lambda \times M_{m2} & \lambda \times M_{m3} & \cdots & \lambda \times M_{mn}
\end{bmatrix}
\]

Uma maneira mais concisa de escrever isso √©:

\[
(\lambda M)_{i,j} = \lambda(M)_{i,j}
\]

No NumPy, basta usar o operador `*` para multiplicar uma matriz por um escalar. Por exemplo:

```run-python
2 * A
```

A multiplica√ß√£o por escalar tamb√©m √© definida no lado direito e d√° o mesmo resultado: \( M\lambda = \lambda M \). Por exemplo:

```run-python
A * 2
```

Isso torna a multiplica√ß√£o por escalar **comutativa**.

Tamb√©m √© **associativa**, o que significa que \( \alpha (\beta M) = (\alpha \times \beta)M \), onde \( \alpha \) e \( \beta \) s√£o escalares. Por exemplo:

```run-python
2 * (3 * A)

(2 * 3) * A
```

Finalmente, √© **distributiva** sobre a adi√ß√£o de matrizes, o que significa que \( \lambda(Q + R) = \lambda Q + \lambda R \):

```run-python
2 * (A + B)

2 * A + 2 * B
```

---

### **Multiplica√ß√£o de matrizes**

At√© agora, as opera√ß√µes matriciais foram bastante intuitivas. Mas multiplicar matrizes √© um pouco mais complexo.

Uma matriz \( Q \) de tamanho \( m \times n \) pode ser multiplicada por uma matriz \( R \) de tamanho \( n \times q \). Ela √© denotada simplesmente por \( QR \) sem sinal de multiplica√ß√£o ou ponto. O resultado \( P \) √© uma matriz \( m \times q \) onde cada elemento √© calculado como uma soma de produtos:

\[
P_{i,j} = \sum_{k=1}^n Q_{i,k} \times R_{k,j}
\]

O elemento na posi√ß√£o \( i, j \) na matriz resultante √© a soma dos produtos dos elementos na linha \( i \) da matriz \( Q \) pelos elementos na coluna \( j \) da matriz \( R \).

\[
P = 
\begin{bmatrix}
Q_{11}R_{11} + Q_{12}R_{21} + \cdots + Q_{1n}R_{n1} & Q_{11}R_{12} + Q_{12}R_{22} + \cdots + Q_{1n}R_{n2} & \cdots \\
Q_{21}R_{11} + Q_{22}R_{21} + \cdots + Q_{2n}R_{n1} & Q_{21}R_{12} + Q_{22}R_{22} + \cdots + Q_{2n}R_{n2} & \cdots \\
\vdots & \vdots & \ddots \\
Q_{m1}R_{11} + Q_{m2}R_{21} + \cdots + Q_{mn}R_{n1} & Q_{m1}R_{12} + Q_{m2}R_{22} + \cdots + Q_{mn}R_{n2} & \cdots
\end{bmatrix}
\]

Voc√™ pode notar que cada elemento \( P_{i,j} \) √© o produto escalar do vetor linha \( Q_{i,*} \) e do vetor coluna \( R_{*,j} \):

\[
P_{i,j} = Q_{i,*} \cdot R_{*,j}
\]

Portanto, podemos reescrever \( P \) de forma mais concisa como:

\[
P = 
\begin{bmatrix}
Q_{1,*} \cdot R_{*,1} & Q_{1,*} \cdot R_{*,2} & \cdots & Q_{1,*} \cdot R_{*,q} \\
Q_{2,*} \cdot R_{*,1} & Q_{2,*} \cdot R_{*,2} & \cdots & Q_{2,*} \cdot R_{*,q} \\
\vdots & \vdots & \ddots & \vdots \\
Q_{m,*} \cdot R_{*,1} & Q_{m,*} \cdot R_{*,2} & \cdots & Q_{m,*} \cdot R_{*,q}
\end{bmatrix}
\]

Vamos multiplicar duas matrizes no NumPy, usando a fun√ß√£o `np.matmul()`:

```run-python
E = np.matmul(A, D)
E
```

O Python 3.5 introduziu o operador infixo `@` para multiplica√ß√£o de matrizes, e o NumPy 1.10 adicionou suporte para ele. `A @ D` √© equivalente a `np.matmul(A, D)`:

```run-python
A @ D
```

O operador `@` tamb√©m funciona para vetores. `u @ v` calcula o produto escalar de `u` e `v`:

```run-python
u @ v
```

Vamos verificar esse resultado olhando para um elemento, s√≥ para ter certeza. Para calcular \( E_{2,3} \), por exemplo, precisamos multiplicar os elementos na 2¬™ linha de \( A \) pelos elementos na 3¬™ coluna de \( D \) e somar esses produtos:

```run-python
40*5 + 50*17 + 60*31

E[1, 2]  # linha 2, coluna 3
```

Parece bom! Voc√™ pode verificar os outros elementos at√© se acostumar com o algoritmo.

Multiplicamos uma matriz \( 2 \times 3 \) por uma matriz \( 3 \times 4 \), ent√£o o resultado √© uma matriz \( 2 \times 4 \). O n√∫mero de colunas da primeira matriz deve ser igual ao n√∫mero de linhas da segunda matriz. Se tentarmos multiplicar \( D \) por \( A \), obteremos um erro porque \( D \) tem 4 colunas enquanto \( A \) tem 2 linhas:

```run-python
try:
    D @ A
except ValueError as e:
    print("ValueError:", e)
```

Isso ilustra o fato de que a multiplica√ß√£o de matrizes **N√ÉO** √© comutativa: em geral, \( QR \neq RQ \).

Na verdade, \( QR \) e \( RQ \) s√£o ambos definidos apenas se \( Q \) tiver tamanho \( m \times n \) e \( R \) tiver tamanho \( n \times m \). Vamos ver um exemplo onde ambos s√£o definidos e mostrar que eles (em geral) **N√ÉO** s√£o iguais:

```run-python
F = np.array([
    [4, 1],
    [9, 3]
])

A @ F

F @ A
```

Por outro lado, a **multiplica√ß√£o de matrizes √© associativa**, o que significa que \( Q(RS) = (QR)S \). Vamos criar uma matriz \( 4 \times 5 \) \( G \) para ilustrar isso:

```run-python
G = np.array([
    [8, 7, 4, 2, 5],
    [2, 5, 1, 0, 5],
    [9, 11, 17, 21, 0],
    [0, 1, 0, 1, 2]
])

(A @ D) @ G  # (AD)G

A @ (D @ G)  # A(DG)
```

Tamb√©m √© **distributiva** sobre a adi√ß√£o de matrizes, o que significa que \( (Q + R)S = QS + RS \). Por exemplo:

```run-python
(A + B) @ D

A @ D + B @ D
```

O produto de uma matriz \( M \) pela matriz identidade (de tamanho correspondente) resulta na mesma matriz \( M \). Mais formalmente, se \( M \) √© uma matriz \( m \times n \), ent√£o:

\[
MI_n = I_mM = M
\]

Isso geralmente √© escrito de forma mais concisa (j√° que o tamanho das matrizes identidade √© inequ√≠voco dado o contexto):

\[
MI = IM = M
\]

Por exemplo:

```run-python
A @ np.eye(3)

np.eye(2) @ A
```

**Cuidado:** O operador `*` do NumPy realiza multiplica√ß√£o elemento por elemento, **N√ÉO** uma multiplica√ß√£o de matrizes:

```run-python
A * B  # N√ÉO √© uma multiplica√ß√£o de matrizes
```

---

### **Transposta de uma matriz**

A transposta de uma matriz \( M \) √© uma matriz denotada por \( M^T \) tal que a \( i^{√©sima} \) linha em \( M^T \) √© igual √† \( i^{√©sima} \) coluna em \( M \):

\[
A^T = 
\begin{bmatrix}
10 & 20 & 30 \\
40 & 50 & 60
\end{bmatrix}^T =
\begin{bmatrix}
10 & 40 \\
20 & 50 \\
30 & 60
\end{bmatrix}
\]

Em outras palavras, \( (A^T)_{i,j} = A_{j,i} \).

Obviamente, se \( M \) √© uma matriz \( m \times n \), ent√£o \( M^T \) √© uma matriz \( n \times m \).

**Nota:** existem algumas outras nota√ß√µes, como \( M^t \), \( M' \) ou \( tM \).

No NumPy, a transposta de uma matriz pode ser obtida simplesmente usando o atributo `T`:

```run-python
A

A.T
```

Como voc√™ pode esperar, transpor uma matriz duas vezes retorna a matriz original:

```run-python
A.T.T
```

A transposi√ß√£o √© distributiva sobre a adi√ß√£o de matrizes, o que significa que \( (Q + R)^T = Q^T + R^T \). Por exemplo:

```run-python
(A + B).T

A.T + B.T
```

Al√©m disso, \( (Q \cdot R)^T = R^T \cdot Q^T \). Observe que a ordem √© invertida. Por exemplo:

```run-python
(A @ D).T

D.T @ A.T
```

---

### **Matriz sim√©trica**

Uma **matriz sim√©trica** √© definida como uma matriz que √© igual √† sua transposta: \( M = M^T \). Essa defini√ß√£o implica que ela deve ser uma matriz quadrada cujos elementos s√£o sim√©tricos em rela√ß√£o √† diagonal principal, por exemplo:

\[
\begin{bmatrix}
17 & 22 & 27 & 49 \\
22 & 29 & 36 & 0 \\
27 & 36 & 45 & 2 \\
49 & 0 & 2 & 99
\end{bmatrix}
\]

O produto de uma matriz por sua transposta √© sempre uma matriz sim√©trica, por exemplo:

```run-python
D @ D.T
```

Como mencionamos anteriormente, no NumPy (ao contr√°rio do Matlab, por exemplo), 1D realmente significa 1D: n√£o existe algo como um array 1D vertical ou horizontal. Portanto, voc√™ n√£o deve se surpreender ao ver que transpor um array 1D n√£o faz nada:

```run-python
u

u.T
```

Queremos converter `u` em um vetor linha antes de transpor. Existem algumas maneiras de fazer isso:

```run-python
u_row = np.array([u])
u_row
```

Observe os colchetes extras: isso √© um array 2D com apenas uma linha (ou seja, uma matriz \( 1 \times 2 \)). Em outras palavras, √© realmente um vetor linha.

```run-python
u[np.newaxis, :]
```

Isso √© bastante expl√≠cito: estamos pedindo um novo eixo vertical, mantendo os dados existentes como o eixo horizontal.

```run-python
u[np.newaxis]
```

Isso √© equivalente, mas um pouco menos expl√≠cito.

```run-python
u[None]
```

Essa √© a vers√£o mais curta, mas voc√™ provavelmente deve evit√°-la porque n√£o √© clara. A raz√£o pela qual funciona √© que `np.newaxis` √© na verdade igual a `None`, ent√£o isso √© equivalente √† vers√£o anterior.

Agora vamos transpor nosso vetor linha:

```run-python
u_row.T
```

√ìtimo! Agora temos um vetor coluna.

Em vez de criar um vetor linha e depois transp√¥-lo, tamb√©m √© poss√≠vel converter um array 1D diretamente em um vetor coluna:

```run-python
u[:, np.newaxis]
```

---

### **Plotando uma matriz**

J√° vimos que vetores podem ser representados como pontos ou setas em um espa√ßo N-dimensional. Existe uma boa representa√ß√£o gr√°fica de matrizes? Bem, voc√™ pode simplesmente ver uma matriz como uma lista de vetores, ent√£o plotar uma matriz resulta em muitos pontos ou setas. Por exemplo, vamos criar uma matriz \( 2 \times 4 \) \( P \) e plot√°-la como pontos:

```run-python
P = np.array([
    [3.0, 4.0, 1.0, 4.6],
    [0.2, 3.5, 2.0, 0.5]
])

x_coords_P, y_coords_P = P
plt.scatter(x_coords_P, y_coords_P)
plt.axis([0, 5, 0, 4])
plt.gca().set_aspect("equal")
plt.grid()
plt.show()
```

Claro, tamb√©m poder√≠amos ter armazenado os mesmos 4 vetores como vetores linha em vez de vetores coluna, resultando em uma matriz \( 4 \times 2 \) (a transposta de \( P \), na verdade). √â realmente uma escolha arbitr√°ria.

Como os vetores s√£o ordenados, voc√™ pode ver a matriz como um caminho e represent√°-la com pontos conectados:

```run-python
plt.plot(x_coords_P, y_coords_P, "bo")
plt.plot(x_coords_P, y_coords_P, "b--")
plt.axis([0, 5, 0, 4])
plt.gca().set_aspect("equal")
plt.grid()
plt.show()
```

Ou voc√™ pode represent√°-la como um pol√≠gono: a classe `Polygon` do `matplotlib` espera um array NumPy \( n \times 2 \), n√£o um array \( 2 \times n \), ent√£o precisamos apenas passar \( P^T \):

```run-python
from matplotlib.patches import Polygon
plt.gca().add_artist(Polygon(P.T))
plt.axis([0, 5, 0, 4])
plt.gca().set_aspect("equal")
plt.grid()
plt.show()
```

---

### **Aplica√ß√µes geom√©tricas de opera√ß√µes matriciais**

Vimos anteriormente que a adi√ß√£o de vetores resulta em uma transla√ß√£o geom√©trica, a multiplica√ß√£o de vetores por um escalar resulta em redimensionamento (zoom in ou out, centrado na origem), e o produto escalar de vetores resulta na proje√ß√£o de um vetor em outro vetor, redimensionando e medindo a coordenada resultante.

Da mesma forma, as opera√ß√µes matriciais t√™m aplica√ß√µes geom√©tricas muito √∫teis.

---

### **Adi√ß√£o = m√∫ltiplas transla√ß√µes geom√©tricas**

Primeiro, adicionar duas matrizes √© equivalente a adicionar todos os seus vetores. Por exemplo, vamos criar uma matriz \( 2 \times 4 \) \( H \) e adicion√°-la a \( P \), e observar o resultado:

```run-python
H = np.array([
    [0.5, -0.2, 0.2, -0.1],
    [0.4, 0.4, 1.5, 0.6]
])

P_moved = P + H

plt.gca().add_artist(Polygon(P.T, alpha=0.2))
plt.gca().add_artist(Polygon(P_moved.T, alpha=0.2))
for vector, origin in zip(H.T, P.T):
    plot_vector2d(vector, origin=origin)

plt.text(2.2, 1.8, "$P$", color="b", fontsize=18)
plt.text(2.0, 3.2, "$P+H$", color="r", fontsize=18)
plt.text(2.5, 0.5, "$H_{*,1}$", color="k", fontsize=18)
plt.text(4.1, 3.5, "$H_{*,2}$", color="k", fontsize=18)
plt.text(0.4, 2.6, "$H_{*,3}$", color="k", fontsize=18)
plt.text(4.3, 0.2, "$H_{*,4}$", color="k", fontsize=18)

plt.axis([0, 5, 0, 4])
plt.gca().set_aspect("equal")
plt.grid()
plt.show()
```

Se adicionarmos uma matriz cheia de vetores id√™nticos, obteremos uma simples transla√ß√£o geom√©trica:

```run-python
H2 = np.array([
    [-0.5, -0.5, -0.5, -0.5],
    [0.4, 0.4, 0.4, 0.4]
])

P_translated = P + H2

plt.gca().add_artist(Polygon(P.T, alpha=0.2))
plt.gca().add_artist(Polygon(P_translated.T, alpha=0.3, color="r"))
for vector, origin in zip(H2.T, P.T):
    plot_vector2d(vector, origin=origin)

plt.axis([0, 5, 0, 4])
plt.gca().set_aspect("equal")
plt.grid()
plt.show()
```

Embora as matrizes s√≥ possam ser somadas se tiverem o mesmo tamanho, o NumPy permite adicionar um vetor linha ou coluna a uma matriz: isso √© chamado de **broadcasting** e √© explicado em mais detalhes no tutorial do NumPy. Poder√≠amos ter obtido o mesmo resultado acima com:

```run-python
P + [[-0.5], [0.4]]  # mesmo que P + H2, gra√ßas ao broadcasting do NumPy
```

---

### **Multiplica√ß√£o por escalar**

Multiplicar uma matriz por um escalar resulta em todos os seus vetores sendo multiplicados por esse escalar, ent√£o, sem surpresa, o resultado geom√©trico √© um redimensionamento de toda a figura. Por exemplo, vamos redimensionar nosso pol√≠gono por um fator de 60% (zoom out, centrado na origem):

```run-python
def plot_transformation(P_before, P_after, text_before, text_after, axis=[0, 5, 0, 4], arrows=True):
    if arrows:
        for vector_before, vector_after in zip(P_before.T, P_after.T):
            plot_vector2d(vector_before, color="blue", linestyle="--")
            plot_vector2d(vector_after, color="red", linestyle="-")
    plt.gca().add_artist(Polygon(P_before.T, alpha=0.2))
    plt.gca().add_artist(Polygon(P_after.T, alpha=0.3, color="r"))
    plt.plot(P_before[0], P_before[1], "b--", alpha=0.5)
    plt.plot(P_after[0], P_after[1], "r--", alpha=0.5)
    plt.text(P_before[0].mean(), P_before[1].mean(), text_before, fontsize=18, color="blue")
    plt.text(P_after[0].mean(), P_after[1].mean(), text_after, fontsize=18, color="red")
    plt.axis(axis)
    plt.gca().set_aspect("equal")
    plt.grid()

P_rescaled = 0.60 * P
plot_transformation(P, P_rescaled, "$P$", "$0.6 P$", arrows=True)
plt.show()
```

---

### **Multiplica√ß√£o de matrizes ‚Äì Proje√ß√£o em um eixo**

A multiplica√ß√£o de matrizes √© mais complexa de visualizar, mas tamb√©m √© a ferramenta mais poderosa dispon√≠vel.

Vamos come√ßar de forma simples, definindo uma matriz \( 1 \times 2 \) \( U = [1 \quad 0] \). Esse vetor linha √© apenas o vetor unit√°rio horizontal.

```run-python
U = np.array([[1, 0]])
```

Agora vamos olhar para o produto escalar \( U \cdot P \):

```run-python
U @ P
```

Essas s√£o as coordenadas horizontais dos vetores em \( P \). Em outras palavras, acabamos de projetar \( P \) no eixo horizontal:

```run-python
def plot_projection(U, P):
    U_P = U @ P
    axis_end = 100 * U

    plt.gca().add_artist(Polygon(P.T, alpha=0.2))
    for vector, proj_coordinate in zip(P.T, U_P.T):
        proj_point = proj_coordinate * U
        plt.plot(proj_point[0][0], proj_point[0][1], "ro", zorder=10)
        plt.plot([vector[0], proj_point[0][0]], [vector[1], proj_point[0][1]], "r--", zorder=10)

    plt.axis([0, 5, 0, 4])
    plt.gca().set_aspect("equal")
    plt.grid()
    plt.show()

plot_projection(U, P)
```

Podemos realmente projetar em qualquer outro eixo, basta substituir \( U \) por qualquer outro vetor unit√°rio. Por exemplo, vamos projetar no eixo que est√° a um √¢ngulo de \( 30^\circ \) acima do eixo horizontal:

```run-python
angle30 = 30 * np.pi / 180  # √¢ngulo em radianos
U_30 = np.array([[np.cos(angle30), np.sin(angle30)]])

plot_projection(U_30, P)
```

Bom! Lembre-se de que o produto escalar de um vetor unit√°rio e uma matriz basicamente realiza uma proje√ß√£o em um eixo e nos d√° as coordenadas dos pontos resultantes nesse eixo.

---

### **Multiplica√ß√£o de matrizes ‚Äì Rota√ß√£o**

Agora vamos criar uma matriz \( 2 \times 2 \) \( V \) contendo dois vetores unit√°rios que fazem √¢ngulos de \( 30^\circ \) e \( 120^\circ \) com o eixo horizontal:

\[
V = 
\begin{bmatrix}
\cos(30^\circ) & \sin(30^\circ) \\
\cos(120^\circ) & \sin(120^\circ)
\end{bmatrix}
\]

```run-python
angle120 = 120 * np.pi / 180
V = np.array([
    [np.cos(angle30), np.sin(angle30)],
    [np.cos(angle120), np.sin(angle120)]
])

V
```

Vamos olhar para o produto \( VP \):

```run-python
V @ P
```

A primeira linha √© igual a \( V_{1,*}P \), que s√£o as coordenadas da proje√ß√£o de \( P \) no eixo de \( 30^\circ \), como vimos acima. A segunda linha √© \( V_{2,*}P \), que s√£o as coordenadas da proje√ß√£o de \( P \) no eixo de \( 120^\circ \). Ent√£o, basicamente, obtivemos as coordenadas de \( P \) ap√≥s girar os eixos horizontal e vertical em \( 30^\circ \) (ou equivalentemente, ap√≥s girar o pol√≠gono em \(-30^\circ\) em torno da origem)! Vamos plotar \( VP \) para ver isso:

```run-python
P_rotated = V @ P
plot_transformation(P, P_rotated, "$P$", "$VP$", [-2, 6, -2, 4], arrows=True)
plt.show()
```

A matriz \( V \) √© chamada de **matriz de rota√ß√£o**.

---

### **Multiplica√ß√£o de matrizes ‚Äì Outras transforma√ß√µes lineares**

De forma mais geral, qualquer transforma√ß√£o linear \( f \) que mapeie vetores n-dimensionais para vetores m-dimensionais pode ser representada como uma matriz \( m \times n \). Por exemplo, digamos que \( \mathbf{u} \) √© um vetor tridimensional:

\[
\mathbf{u} = 
\begin{pmatrix}
x \\
y \\
z
\end{pmatrix}
\]

e \( f \) √© definida como:

\[
f(\mathbf{u}) =
\begin{pmatrix}
a x + b y + c z \\
d x + e y + f z
\end{pmatrix}
\]

Essa transforma√ß√£o \( f \) mapeia vetores tridimensionais para vetores bidimensionais de forma linear (ou seja, as coordenadas resultantes envolvem apenas somas de m√∫ltiplos das coordenadas originais). Podemos representar essa transforma√ß√£o como a matriz \( F \):

\[
F = 
\begin{bmatrix}
a & b & c \\
d & e & f
\end{bmatrix}
\]

Agora, para calcular \( f(\mathbf{u}) \), podemos simplesmente fazer uma multiplica√ß√£o de matrizes:

\[
f(\mathbf{u}) = F \mathbf{u}
\]

Se tivermos uma matriz \( G = [\mathbf{u}_1 \quad \mathbf{u}_2 \quad \cdots \quad \mathbf{u}_q] \), onde cada \( \mathbf{u}_i \) √© um vetor coluna tridimensional, ent√£o \( FG \) resulta na transforma√ß√£o linear de todos os vetores \( \mathbf{u}_i \) conforme definido pela matriz \( F \):

\[
FG = [f(\mathbf{u}_1) \quad f(\mathbf{u}_2) \quad \cdots \quad f(\mathbf{u}_q)]
\]

Para resumir, a matriz no lado esquerdo de um produto escalar especifica qual transforma√ß√£o linear aplicar aos vetores do lado direito. J√° mostramos que isso pode ser usado para realizar proje√ß√µes e rota√ß√µes, mas qualquer outra transforma√ß√£o linear √© poss√≠vel. Por exemplo, aqui est√° uma transforma√ß√£o conhecida como **mapeamento de cisalhamento**:

```run-python
F_shear = np.array([
    [1, 1.5],
    [0, 1]
])

plot_transformation(P, F_shear @ P, "$P$", "$F_{\text{shear}} P$", axis=[0, 10, 0, 7])
plt.show()
```

Vamos ver como essa transforma√ß√£o afeta o quadrado unit√°rio:

```run-python
Square = np.array([
    [0, 0, 1, 1],
    [0, 1, 1, 0]
])

plot_transformation(Square, F_shear @ Square, "$Square$", "$F_{\text{shear}} Square$", axis=[0, 2.6, 0, 1.8])
plt.show()
```

Agora vamos ver um **mapeamento de compress√£o**:

```run-python
F_squeeze = np.array([
    [1.4, 0],
    [0, 1/1.4]
])

plot_transformation(P, F_squeeze @ P, "$P$", "$F_{\text{squeeze}} P$", axis=[0, 7, 0, 5])
plt.show()
```

O efeito no quadrado unit√°rio √©:

```run-python
plot_transformation(Square, F_squeeze @ Square, "$Square$", "$F_{\text{squeeze}} Square$", axis=[0, 1.8, 0, 1.2])
plt.show()
```

Vamos mostrar mais uma ‚Äì reflex√£o atrav√©s do eixo horizontal:

```run-python
F_reflect = np.array([
    [1, 0],
    [0, -1]
])

plot_transformation(P, F_reflect @ P, "$P$", "$F_{\text{reflect}} P$", axis=[-2, 9, -4.5, 4.5])
plt.show()
```

---

### **Inversa de uma matriz**

Agora que entendemos que uma matriz pode representar qualquer transforma√ß√£o linear, uma pergunta natural √©: podemos encontrar uma matriz de transforma√ß√£o que reverta o efeito de uma determinada matriz de transforma√ß√£o \( F \)? A resposta √© sim... √†s vezes! Quando existe, essa matriz √© chamada de **inversa** de \( F \), e √© denotada por \( F^{-1} \).

Por exemplo, a rota√ß√£o, o mapeamento de cisalhamento e o mapeamento de compress√£o acima t√™m transforma√ß√µes inversas. Vamos demonstrar isso no mapeamento de cisalhamento:

```run-python
F_inv_shear = np.array([
    [1, -1.5],
    [0, 1]
])

P_sheared = F_shear @ P
P_unsheared = F_inv_shear @ P_sheared
plot_transformation(P_sheared, P_unsheared, "$P_{\text{sheared}}$", "$P_{\text{unsheared}}$", axis=[0, 10, 0, 7])
plt.plot(P[0], P[1], "b--")
plt.show()
```

Aplicamos um mapeamento de cisalhamento em \( P \), como fizemos antes, mas ent√£o aplicamos uma segunda transforma√ß√£o ao resultado, e _voil√†_, isso teve o efeito de voltar ao \( P \) original (plotei o contorno do \( P \) original para verificar). A segunda transforma√ß√£o √© a inversa da primeira.

Definimos a matriz inversa \( F^{-1}_{\text{shear}} \) manualmente desta vez, mas o NumPy fornece uma fun√ß√£o `inv` para calcular a inversa de uma matriz, ent√£o poder√≠amos ter escrito:

```run-python
F_inv_shear = LA.inv(F_shear)
F_inv_shear
```

Apenas matrizes quadradas podem ser invertidas. Isso faz sentido quando voc√™ pensa sobre isso: se voc√™ tiver uma transforma√ß√£o que reduz o n√∫mero de dimens√µes, ent√£o algumas informa√ß√µes s√£o perdidas e n√£o h√° como recuper√°-las. Por exemplo, digamos que voc√™ use uma matriz \( 2 \times 3 \) para projetar um objeto 3D em um plano. O resultado pode parecer com isso:

```run-python
plt.plot([0, 0, 1, 1, 0, 0.1, 0.1, 0, 0.1, 1.1, 1.0, 1.1, 1.1, 1.0, 1.1, 0.1],
         [0, 1, 1, 0, 0, 0.1, 1.1, 1.0, 1.1, 1.1, 1.0, 1.1, 0.1, 0, 0.1, 0.1],
         "r-")
plt.axis([-0.5, 2.1, -0.5, 1.5])
plt.gca().set_aspect("equal")
plt.grid()
plt.show()
```

Olhando para essa imagem, √© imposs√≠vel dizer se isso √© a proje√ß√£o de um cubo ou a proje√ß√£o de um objeto retangular estreito. Algumas informa√ß√µes foram perdidas na proje√ß√£o.

Mesmo matrizes de transforma√ß√£o quadradas podem perder informa√ß√µes. Por exemplo, considere esta matriz de transforma√ß√£o:

```run-python
F_project = np.array([
    [1, 0],
    [0, 0]
])

plot_transformation(P, F_project @ P, "$P$", r"$F_{\text{project}} \cdot P$", axis=[0, 6, -1, 4])
plt.show()
```

Essa matriz de transforma√ß√£o realiza uma proje√ß√£o no eixo horizontal. Nosso pol√≠gono √© completamente achatado, ent√£o algumas informa√ß√µes s√£o totalmente perdidas, e √© imposs√≠vel voltar ao pol√≠gono original usando uma transforma√ß√£o linear. Em outras palavras, \( F_{\text{project}} \) n√£o tem inversa. Tal matriz quadrada que n√£o pode ser invertida √© chamada de **matriz singular** (tamb√©m conhecida como matriz degenerada). Se pedirmos ao NumPy para calcular sua inversa, ele levantar√° uma exce√ß√£o:

```run-python
try:
    LA.inv(F_project)
except LA.LinAlgError as e:
    print("LinAlgError:", e)
```

Aqui est√° outro exemplo de uma matriz singular. Esta realiza uma proje√ß√£o no eixo a um √¢ngulo de \( 30^\circ \) acima do eixo horizontal:

```run-python
angle30 = 30 * np.pi / 180
F_project_30 = np.array([
    [np.cos(angle30)**2, np.sin(2*angle30)/2],
    [np.sin(2*angle30)/2, np.sin(angle30)**2]
])

plot_transformation(P, F_project_30 @ P, "$P$", r"$F_{\text{project\_30}} \cdot P$", axis=[0, 6, -1, 4])
plt.show()
```

Mas desta vez, devido a erros de arredondamento de ponto flutuante, o NumPy consegue calcular uma inversa (observe como os elementos s√£o grandes, no entanto):

```run-python
LA.inv(F_project_30)
```

Como voc√™ pode esperar, o produto escalar de uma matriz por sua inversa resulta na matriz identidade:

\[
M \cdot M^{-1} = M^{-1} \cdot M = I
\]

Isso faz sentido, j√° que fazer uma transforma√ß√£o linear seguida pela transforma√ß√£o inversa resulta em nenhuma mudan√ßa.

```run-python
F_shear @ LA.inv(F_shear)
```

Outra maneira de expressar isso √© que a inversa da inversa de uma matriz \( M \) √© \( M \) mesma:

\[
((M)^{-1})^{-1} = M
\]

```run-python
LA.inv(LA.inv(F_shear))
```

Al√©m disso, a inversa de escalonar por um fator de \( \lambda \) √©, √© claro, escalonar por um fator de \( \frac{1}{\lambda} \):

\[
(\lambda \times M)^{-1} = \frac{1}{\lambda} \times M^{-1}
\]

Assim que voc√™ entende a interpreta√ß√£o geom√©trica das matrizes como transforma√ß√µes lineares, a maioria dessas propriedades parece bastante intuitiva.

Uma matriz que √© sua pr√≥pria inversa √© chamada de **involu√ß√£o**. Os exemplos mais simples s√£o matrizes de reflex√£o, ou uma rota√ß√£o de \( 180^\circ \), mas tamb√©m existem involu√ß√µes mais complexas, por exemplo, imagine uma transforma√ß√£o que comprime horizontalmente, depois reflete sobre o eixo vertical e finalmente gira \( 90^\circ \) no sentido hor√°rio. Pegue um guardanapo e tente fazer isso duas vezes: voc√™ acabar√° na posi√ß√£o original. Aqui est√° a matriz involut√≥ria correspondente:

```run-python
F_involution = np.array([
    [0, -2],
    [-1/2, 0]
])

plot_transformation(P, F_involution @ P, "$P$", r"$F_{\text{involution}} \cdot P$", axis=[-8, 5, -4, 4])
plt.show()
```

Finalmente, uma matriz quadrada \( H \) cuja inversa √© sua pr√≥pria transposta √© uma **matriz ortogonal**:

\[
H^{-1} = H^T
\]

Portanto:

\[
H \cdot H^T = H^T \cdot H = I
\]

Ela corresponde a uma transforma√ß√£o que preserva dist√¢ncias, como rota√ß√µes e reflex√µes, e combina√ß√µes dessas, mas n√£o redimensionamento, cisalhamento ou compress√£o. Vamos verificar que \( F_{\text{reflect}} \) √© de fato ortogonal:

```run-python
F_reflect @ F_reflect.T
```

---

### **Determinante**

O determinante de uma matriz quadrada \( M \), denotado por \( \det(M) \) ou \( \det M \) ou \( |M| \), √© um valor que pode ser calculado a partir de seus elementos \( (M_{i,j}) \) usando v√°rios m√©todos equivalentes. Um dos m√©todos mais simples √© esta abordagem recursiva:

\[
|M| = M_{1,1} \times |M^{(1,1)}| - M_{1,2} \times |M^{(1,2)}| + M_{1,3} \times |M^{(1,3)}| - M_{1,4} \times |M^{(1,4)}| + \cdots \pm M_{1,n} \times |M^{(1,n)}|
\]

- Onde \( M^{(i,j)} \) √© a matriz \( M \) sem a linha \( i \) e a coluna \( j \).

Por exemplo, vamos calcular o determinante da seguinte matriz \( 3 \times 3 \):

\[
M = 
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 0
\end{bmatrix}
\]

Usando o m√©todo acima, obtemos:

\[
|M| = 1 \times |[5 \quad 6 \quad 8 \quad 0]| - 2 \times |[4 \quad 6 \quad 7 \quad 0]| + 3 \times |[4 \quad 5 \quad 7 \quad 8]|
\]

Agora precisamos calcular o determinante de cada uma dessas matrizes \( 2 \times 2 \) (esses determinantes s√£o chamados de menores):

\[
\begin{vmatrix}
5 & 6 \\
8 & 0
\end{vmatrix} = 5 \times 0 - 6 \times 8 = -48
\]

\[
\begin{vmatrix}
4 & 6 \\
7 & 0
\end{vmatrix} = 4 \times 0 - 6 \times 7 = -42
\]

\[
\begin{vmatrix}
4 & 5 \\
7 & 8
\end{vmatrix} = 4 \times 8 - 5 \times 7 = -3
\]

Agora podemos calcular o resultado final:

\[
|M| = 1 \times (-48) - 2 \times (-42) + 3 \times (-3) = 27
\]

Para obter o determinante de uma matriz, voc√™ pode chamar a fun√ß√£o `det` do NumPy no m√≥dulo `numpy.linalg`:

```run-python
M = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 0]
])

LA.det(M)
```

Um dos principais usos do determinante √© determinar se uma matriz quadrada pode ser invertida ou n√£o: se o determinante for igual a 0, ent√£o a matriz n√£o pode ser invertida (√© uma matriz singular), e se o determinante n√£o for 0, ent√£o ela pode ser invertida.

Por exemplo, vamos calcular o determinante para as matrizes \( F_{\text{project}} \), \( F_{\text{project\_30}} \) e \( F_{\text{shear}} \) que definimos anteriormente:

```run-python
LA.det(F_project)
```

Isso mesmo, \( F_{\text{project}} \) √© singular, como vimos anteriormente.

```run-python
LA.det(F_project_30)
```

Esse determinante est√° suspeitamente pr√≥ximo de 0: ele realmente deveria ser 0, mas n√£o √© devido a pequenos erros de ponto flutuante. A matriz √© na verdade singular.

```run-python
LA.det(F_shear)
```

Perfeito! Essa matriz pode ser invertida, como vimos anteriormente. Uau, a matem√°tica realmente funciona!

O determinante tamb√©m pode ser usado para medir o quanto uma transforma√ß√£o linear afeta as √°reas de superf√≠cie: por exemplo, as matrizes de proje√ß√£o \( F_{\text{project}} \) e \( F_{\text{project\_30}} \) achatam completamente o pol√≠gono \( P \), at√© que sua √°rea seja zero. √â por isso que o determinante dessas matrizes √© 0. O mapeamento de cisalhamento modificou a forma do pol√≠gono, mas n√£o afetou sua √°rea de superf√≠cie, e √© por isso que o determinante √© 1. Voc√™ pode tentar calcular o determinante de uma matriz de rota√ß√£o e tamb√©m deve encontrar 1. E quanto a uma matriz de redimensionamento? Vamos ver:

```run-python
F_scale = np.array([
    [0.5, 0],
    [0, 0.5]
])

plot_transformation(P, F_scale @ P, "$P$", r"$F_{\text{scale}} \cdot P$", axis=[0, 6, -1, 4])
plt.show()
```

Redimensionamos o pol√≠gono por um fator de 1/2 em ambos os eixos vertical e horizontal, ent√£o a √°rea de superf√≠cie do pol√≠gono resultante √© \( 1/4 \) da √°rea do pol√≠gono original. Vamos calcular o determinante e verificar isso:

```run-python
LA.det(F_scale)
```

Correto!

O determinante pode realmente ser negativo, quando a transforma√ß√£o resulta em uma vers√£o "invertida" do pol√≠gono original (por exemplo, uma luva para a m√£o esquerda se torna uma luva para a m√£o direita). Por exemplo, o determinante da matriz \( F_{\text{reflect}} \) √© -1 porque a √°rea de superf√≠cie √© preservada, mas o pol√≠gono √© invertido:

```run-python
LA.det(F_reflect)
```

---

### **Compondo transforma√ß√µes lineares**

V√°rias transforma√ß√µes lineares podem ser encadeadas simplesmente realizando m√∫ltiplos produtos escalares em sequ√™ncia. Por exemplo, para realizar um mapeamento de compress√£o seguido por um mapeamento de cisalhamento, basta escrever:

```run-python
P_squeezed_then_sheared = F_shear @ (F_squeeze @ P)
```

Como o produto escalar √© associativo, o seguinte c√≥digo √© equivalente:

```run-python
P_squeezed_then_sheared = F_shear @ F_squeeze @ P
```

Observe que a ordem das transforma√ß√µes √© o inverso da ordem do produto escalar.

Se formos realizar essa composi√ß√£o de transforma√ß√µes lineares mais de uma vez, podemos salvar a matriz de composi√ß√£o assim:

```run-python
F_squeeze_then_shear = F_shear @ F_squeeze
P_squeezed_then_sheared = F_squeeze_then_shear @ P
```

A partir de agora, podemos realizar ambas as transforma√ß√µes em apenas um produto escalar, o que pode levar a um aumento significativo no desempenho.

E se voc√™ quiser realizar o inverso dessa dupla transforma√ß√£o? Bem, se voc√™ comprimiu e depois cisalhou, e quer desfazer o que fez, deve ser √≥bvio que voc√™ deve primeiro "descisalhar" e depois "descomprimir". Em termos mais matem√°ticos, dadas duas matrizes invert√≠veis (tamb√©m conhecidas como n√£o singulares) \( Q \) e \( R \):

\[
(Q \cdot R)^{-1} = R^{-1} \cdot Q^{-1}
\]

E no NumPy:

```run-python
LA.inv(F_shear @ F_squeeze) == LA.inv(F_squeeze) @ LA.inv(F_shear)
```

---

### **Decomposi√ß√£o em Valores Singulares (SVD)**

Acontece que qualquer matriz \( m \times n \) \( M \) pode ser decomposta no produto escalar de tr√™s matrizes simples:

- Uma matriz de rota√ß√£o \( U \) (uma matriz ortogonal \( m \times m \))
- Uma matriz de escalonamento e proje√ß√£o \( \Sigma \) (uma matriz diagonal \( m \times n \))
- E outra matriz de rota√ß√£o \( V^T \) (uma matriz ortogonal \( n \times n \))

\[
M = U \cdot \Sigma \cdot V^T
\]

Por exemplo, vamos decompor a transforma√ß√£o de cisalhamento:

```run-python
U, S_diag, V_T = LA.svd(F_shear)  # nota: no Python 3, voc√™ pode renomear S_diag para Œ£_diag
S_diag
```

Observe que isso √© apenas um array 1D contendo os valores diagonais de \( \Sigma \). Para obter a matriz \( \Sigma \) real, podemos usar a fun√ß√£o `diag` do NumPy:

```run-python
S = np.diag(S_diag)
S
```

Agora vamos verificar que \( U \cdot \Sigma \cdot V^T \) √© de fato igual a \( F_{\text{shear}} \):

```run-python
U @ np.diag(S_diag) @ V_T

F_shear
```

Funcionou perfeitamente. Vamos aplicar essas transforma√ß√µes uma por uma (em ordem inversa) no quadrado unit√°rio para entender o que est√° acontecendo. Primeiro, vamos aplicar a primeira rota√ß√£o \( V^T \):

```run-python
plot_transformation(Square, V_T @ Square, "$Square$", r"$V^T \cdot Square$", axis=[-0.5, 3.5, -1.5, 1.5])
plt.show()
```

Agora vamos redimensionar ao longo dos eixos vertical e horizontal usando \( \Sigma \):

```run-python
plot_transformation(V_T @ Square, S @ V_T @ Square, r"$V^T \cdot Square$", r"$\Sigma \cdot V^T \cdot Square$", axis=[-0.5, 3.5, -1.5, 1.5])
plt.show()
```

Finalmente, aplicamos a segunda rota√ß√£o \( U \):

```run-python
plot_transformation(S @ V_T @ Square, U @ S @ V_T @ Square, r"$\Sigma \cdot V^T \cdot Square$", r"$U \cdot \Sigma \cdot V^T \cdot Square$", axis=[-0.5, 3.5, -1.5, 1.5])
plt.show()
```

E podemos ver que o resultado √© de fato um mapeamento de cisalhamento do quadrado unit√°rio original.

---

### **Autovetores e autovalores**

Um **autovetor** de uma matriz quadrada \( M \) (tamb√©m chamado de **vetor caracter√≠stico**) √© um vetor n√£o nulo que permanece na mesma linha ap√≥s a transforma√ß√£o pela transforma√ß√£o linear associada a \( M \). Uma defini√ß√£o mais formal √© qualquer vetor \( \mathbf{v} \) tal que:

\[
M \cdot \mathbf{v} = \lambda \times \mathbf{v}
\]

Onde \( \lambda \) √© um valor escalar chamado de **autovalor** associado ao vetor \( \mathbf{v} \).

Por exemplo, qualquer vetor horizontal permanece horizontal ap√≥s aplicar o mapeamento de cisalhamento (como voc√™ pode ver na imagem acima), ent√£o ele √© um autovetor de \( M \). Um vetor vertical acaba inclinado para a direita, ent√£o vetores verticais **N√ÉO** s√£o autovetores de \( M \).

Se olharmos para o mapeamento de compress√£o, descobrimos que qualquer vetor horizontal ou vertical mant√©m sua dire√ß√£o (embora seu comprimento mude), ent√£o todos os vetores horizontais e verticais s√£o autovetores de \( F_{\text{squeeze}} \).

No entanto, matrizes de rota√ß√£o n√£o t√™m autovetores (exceto se o √¢ngulo de rota√ß√£o for \( 0^\circ \) ou \( 180^\circ \), caso em que todos os vetores n√£o nulos s√£o autovetores).

A fun√ß√£o `eig` do NumPy retorna a lista de autovetores unit√°rios e seus autovalores correspondentes para qualquer matriz quadrada. Vamos ver os autovetores e autovalores da matriz de compress√£o \( F_{\text{squeeze}} \):

```run-python
eigenvalues, eigenvectors = LA.eig(F_squeeze)
eigenvalues  # [Œª0, Œª1, ‚Ä¶]

eigenvectors  # [v0, v1, ‚Ä¶]
```

De fato, os vetores horizontais s√£o esticados por um fator de 1,4, e os vetores verticais s√£o comprimidos por um fator de 1/1,4=0,714..., ent√£o tudo bem. Vamos ver a matriz de cisalhamento \( F_{\text{shear}} \):

```run-python
eigenvalues2, eigenvectors2 = LA.eig(F_shear)
eigenvalues2  # [Œª0, Œª1, ‚Ä¶]

eigenvectors2  # [v0, v1, ‚Ä¶]
```

Espere, o qu√™!? Esper√°vamos apenas um autovetor unit√°rio, n√£o dois. O segundo vetor √© quase igual a \( \begin{pmatrix} -1 \\ 0 \end{pmatrix} \), que est√° na mesma linha que o primeiro vetor \( \begin{pmatrix} 1 \\ 0 \end{pmatrix} \). Isso se deve a erros de ponto flutuante. Podemos ignorar com seguran√ßa vetores que s√£o (quase) colineares (ou seja, na mesma linha).

---

### **Tra√ßo**

O tra√ßo de uma matriz quadrada \( M \), denotado por \( \text{tr}(M) \), √© a soma dos valores em sua diagonal principal. Por exemplo:

```run-python
D = np.array([
    [100, 200, 300],
    [10, 20, 30],
    [1, 2, 3]
])

D.trace()
```

O tra√ßo n√£o tem uma interpreta√ß√£o geom√©trica simples (em geral), mas tem v√°rias propriedades que o tornam √∫til em muitas √°reas:

- \( \text{tr}(A + B) = \text{tr}(A) + \text{tr}(B) \)
- \( \text{tr}(A \cdot B) = \text{tr}(B \cdot A) \)
- \( \text{tr}(A \cdot B \cdot \cdots Y \cdot Z) = \text{tr}(Z \cdot A \cdot B \cdot \cdots Y) \)
- \( \text{tr}(A^T \cdot B) = \text{tr}(A \cdot B^T) = \text{tr}(B^T \cdot A) = \text{tr}(B \cdot A^T) = \sum_{i,j} X_{i,j} \times Y_{i,j} \)

No entanto, ele tem uma interpreta√ß√£o geom√©trica √∫til no caso de matrizes de proje√ß√£o (como \( F_{\text{project}} \) que discutimos anteriormente): ele corresponde ao n√∫mero de dimens√µes ap√≥s a proje√ß√£o. Por exemplo:

```run-python
F_project.trace()
```

---

### **O que vem a seguir?**

Isso conclui esta introdu√ß√£o √† √Ålgebra Linear. Embora esses conceitos b√°sicos cubram a maior parte do que voc√™ precisar√° saber para Aprendizado de M√°quina, se voc√™ quiser se aprofundar neste t√≥pico, h√° muitas op√ß√µes dispon√≠veis: livros de √Ålgebra Linear, li√ß√µes da Khan Academy ou apenas p√°ginas da Wikipedia.

---

**Fim do conte√∫do**

---

A tradu√ß√£o foi feita mantendo a formata√ß√£o original e sem traduzir as partes de c√≥digo. Se precisar de mais alguma coisa, estou √† disposi√ß√£o!






